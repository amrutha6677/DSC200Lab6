# What I have so far, not finished
import pandas as pd


def merge_datasets(dataset1_url, dataset2_url, dataset3_url, output_file):
    chunk_size = 10000  # Adjust the chunk size as needed

    dataset1_chunks = pd.read_csv(dataset1_url, chunksize=chunk_size, engine='python')

    dataset2_chunks = pd.read_csv(dataset2_url, chunksize=chunk_size, engine='python')

    dataset3_chunks = pd.read_csv(dataset3_url, chunksize=chunk_size, engine='python')

    merged_chunks = []

    for chunk1, chunk2, chunk3 in zip(dataset1_chunks, dataset2_chunks, dataset3_chunks):
        # Replace problematic data with NaN
        chunk2['INSTANCEID'] = chunk2['INSTANCEID'].apply(
            lambda x: float('nan') if x == "IMPROPERLY DISCHARGING FIREARM AT/INTO HABITATION/SCHOOL" else x)

        merged_chunk = pd.concat([chunk1, chunk2, chunk3], axis=0)
        merged_chunks.append(merged_chunk)

    merged_data = pd.concat(merged_chunks, axis=0)

    merged_data.to_csv(output_file, index=False)

if __name__ == "__main__":
    dataset1_url = "C:\DSC 200\Crimes_-_2001_to_Present.csv"
    dataset2_url = "C:\DSC 200\PDI__Police_Data_Initiative__Crime_Incidents.csv"
    dataset3_url = "C:\DSC 200\PDI__Police_Data_Initiative__Crime_Incidents.csv"
    output_file = "merged_dataset.csv"

    merge_datasets(dataset1_url, dataset2_url, dataset3_url, output_file)



